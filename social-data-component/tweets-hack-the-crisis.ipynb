{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hack the Crisis\n",
    "\n",
    "Twitter data component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset/swe_tweets.csv\")\n",
    "data['timestamp'] =  pd.to_datetime(data['timestamp'])\n",
    "data = data[['timestamp','tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daydata = data.groupby([data['timestamp'].dt.date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day,data in daydata:\n",
    "    with open(\"docs/\" + str(day) + \".txt\", \"w\") as docfile:\n",
    "        for t in data.tweet:\n",
    "            docfile.write(str(t) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment scores per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare categorisation\n",
    "with open('scoring_tools/categories.pickle', 'rb') as handle:\n",
    "    category_dict = pickle.load(handle)\n",
    "with open('scoring_tools/wordfeatures.pickle', 'rb') as handle:\n",
    "    wordfeatures_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Swedish translation (only of the used categories)\n",
    "transl_df = pd.read_csv(\"scoring_tools/swe_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwords = transl_df.en.tolist()\n",
    "swewords = transl_df.sv.tolist()\n",
    "transl_dict = dict(zip(enwords, swewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swedish_scoring = {}\n",
    "\n",
    "for w in enwords:\n",
    "    k = transl_dict.get(w)\n",
    "    v = wordfeatures_dict.get(w)\n",
    "    swedish_scoring[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the daily docs\n",
    "days = [f.split(\"/\")[1].split(\".\")[0] for f in glob.glob(\"docs/*.txt\")]\n",
    "docs = [open(file,\"r\").read() for file in glob.glob(\"docs/*.txt\")]\n",
    "docs = [re.sub(\"\\n\",\" \",d) for d in docs]\n",
    "daydocs = pd.DataFrame(zip(days,docs))\n",
    "daydocs.columns=['day','text']\n",
    "daydocs = daydocs.sort_values(by=\"day\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = daydocs.day.tolist()\n",
    "docs = daydocs.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for c,doc in enumerate(docs):\n",
    "    print(\"\\r\" + str(c+1) + \"/\" + str(len(docs)), end=\"\")\n",
    "        \n",
    "    data_dict = {}\n",
    "    \n",
    "    data_dict['doc'] = doc\n",
    "    doc = doc.split()\n",
    "    doc = [w.lower().strip() for w in doc]\n",
    "    doc = [re.sub(\"\\.|,|\\:|/|\\\"|\\?|-|â€¦|'|\\(|\\)|\\!|\\+\",\"\", w) for w in doc]\n",
    "\n",
    "    docfeatures = []\n",
    "    for w in doc:\n",
    "        if w in swedish_scoring:\n",
    "            feats = swedish_scoring[w]\n",
    "            try:\n",
    "                for f in feats:\n",
    "                    docfeatures.append(f)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    for df in docfeatures:\n",
    "        label = category_dict[df]\n",
    "        label = label.split()[0]\n",
    "        \n",
    "    counts = Counter(docfeatures)\n",
    "    for k,v in counts.items():\n",
    "        category = category_dict[k]\n",
    "        category = category.split()[0]\n",
    "        count = v\n",
    "        proportion_by_wordcount = (v/len(doc))*100 # this gives same scores as in LIWC's own software\n",
    "        \n",
    "        data_dict[category] = round(proportion_by_wordcount, 2)\n",
    "    \n",
    "    dataset.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame.from_records(dataset).fillna(0)\n",
    "scores_df['day'] = days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = scores_df[['day',\n",
    "                    'health',\n",
    "                    'death',\n",
    "                    'bio',\n",
    "                    'body',\n",
    "                    'affect',\n",
    "                    'anger',\n",
    "                    'swear',\n",
    "                    'anx',\n",
    "                    'sad',\n",
    "                    'feel',\n",
    "                    'friend',\n",
    "                    'family',\n",
    "                    'social',\n",
    "                    'money',\n",
    "                    'work',\n",
    "                    'relig',\n",
    "                    'power',\n",
    "                    'cause',\n",
    "                    'certain',\n",
    "                    'insight',\n",
    "                    'compare',\n",
    "                    'risk',\n",
    "                    'interrog',\n",
    "                    'focusfuture',\n",
    "                    'tentat',\n",
    "                    'quant']\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add VADER posemo and negemo\n",
    "posemo = []\n",
    "negemo = []\n",
    "\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for c,doc in enumerate(docs):\n",
    "    docpos = []\n",
    "    docneg = []\n",
    "    \n",
    "    sents = sent_tokenize(doc)\n",
    "    for sent in sents:\n",
    "        vs = analyzer.polarity_scores(sent)\n",
    "        docpos.append(vs.get('pos'))\n",
    "        docneg.append(vs.get('neg'))\n",
    "    docpos = sum(docpos)/len(docpos)\n",
    "    docneg = sum(docneg)/len(docneg)\n",
    "    posemo.append(docpos)\n",
    "    negemo.append(docneg)\n",
    "    print(\"\\r\" + str(c+1) + \"/\" + str(len(docs)), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df['posemo'] = posemo\n",
    "scores_df['negemo'] = negemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[\"diseaseIndex\"] = scores_df.health+scores_df.death+scores_df.bio+scores_df.body\n",
    "scores_df[\"emotionIndex\"] = scores_df.negemo+scores_df.affect+scores_df.anger+scores_df.swear+scores_df.anx+scores_df.sad+scores_df.posemo+scores_df.feel\n",
    "scores_df[\"relationIndex\"] = scores_df.friend+scores_df.family+scores_df.social\n",
    "scores_df[\"economyIndex\"] = scores_df.money+scores_df.work\n",
    "scores_df[\"politicalIndex\"] = scores_df.relig+scores_df.power+scores_df.cause+scores_df.certain+scores_df.insight+scores_df.compare+scores_df.risk+scores_df.interrog+scores_df.focusfuture+scores_df.tentat+scores_df.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_norm = list(scores_df.columns)\n",
    "cols_to_norm.remove('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[cols_to_norm] = scores_df[cols_to_norm].apply(lambda x: 2*(x - x.min()) / (x.max() - x.min())-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.posemo.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.insight.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.relationIndex.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv(\"tabular-data-output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse to fit frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = {}\n",
    "\n",
    "# the key to append to the dict\n",
    "for (idx,row) in scores_df.iterrows():\n",
    "    twitter_data[row.day]={\n",
    "        \"disease\": {\n",
    "            \"diseaseIndex\":row.diseaseIndex,\n",
    "            \"health\":row.health,\n",
    "            \"death\":row.death,\n",
    "            \"bio\":row.bio,\n",
    "            \"body\":row.body\n",
    "        },\n",
    "        \n",
    "        \"emotion\": {\n",
    "            \"emotionIndex\":row.emotionIndex,\n",
    "            \"feel\":row.feel,\n",
    "            \"negemo\":row.negemo,\n",
    "            \"posemo\":row.posemo,\n",
    "            \"affect\":row.affect,\n",
    "            \"swear\":row.swear,\n",
    "            \"anxiety\":row.anx,\n",
    "            \"sad\":row.sad,\n",
    "            \"anger\":row.anger\n",
    "        },\n",
    "        \n",
    "        \"relation\": {\n",
    "            \"relationIndex\":row.relationIndex,\n",
    "            \"friend\":row.friend,\n",
    "            \"family\":row.family,\n",
    "            \"social\":row.social\n",
    "        },\n",
    "        \n",
    "        \"economy\": {\n",
    "            \"economyIndex\":row.economyIndex,\n",
    "            \"money\":row.money,\n",
    "            \"work\":row.work\n",
    "        },\n",
    "        \n",
    "         \"political\": {\n",
    "            \"politicalIndex\":row.politicalIndex,\n",
    "            \"power\":row.power,\n",
    "            \"cause\":row.cause,\n",
    "            \"certain\":row.certain,\n",
    "            \"insight\":row.insight,\n",
    "            \"compare\":row.compare,\n",
    "            \"risk\":row.risk,\n",
    "            \"interrog\":row.interrog,\n",
    "            \"focusfuture\":row.focusfuture,\n",
    "            \"relig\":row.relig,\n",
    "            \"tentat\":row.tentat,\n",
    "            \"quant\":row.quant\n",
    "         }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data-delivery-to-frontend.txt', 'w') as file:\n",
    "    file.write(str(twitter_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
